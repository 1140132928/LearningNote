# 学习目标

+ 解决的问题
+ 使用的方法（原理核心）
+ 应用的场景

# 学习策略

**以用促学**：找一个小项目，自己动手写代码，然后不断去学习不懂的知识。纯粹任务驱动。

学习策略：（博客笔记时刻准备）

+ 初级：先快速把书籍过一遍，配合视频讲解，建立基本框架，对问题算法有直观理解，然后再翻阅数据，手动推理公式，实现代码。
+ 中级：比较各类算法的不同，适用场景。参加比赛和做项目，以用促学。
+ 高级：思考算法为什么这样推导，多问为什么?

## 较好的任务驱动平台

+ Kaggle：一个数据竞赛平台，有很多教程，项目，加深对算法理解。
+ [metacademy](https://metacademy.org/)：知识图谱，将知识依赖关系绘制出来，提供很好的入门学习指导。包括对应知识的优质学习资源。
+ [吴恩达机器学习](https://www.coursera.org/learn/machine-learning)：深入浅出的神课，包含许多课后作业，可手动实现
  + 课后习题实现：[python](https://github.com/nsoojin/coursera-ml-py)[|jupyter](https://github.com/kaleko/CourseraML)
  + [博士个人笔记](https://github.com/fengdu78/Coursera-ML-AndrewNg-Notes)
  + [octave文档](https://octave.org/doc/v5.2.0/)：类似 matlab 的开源计算软件，有[在线使用版本](https://octave-online.net/)；[wiki](http://wiki.octave.org/)
+ [谷歌出品的机器学习课程](https://developers.google.com/machine-learning/crash-course)：快速入门，实例演练
+ 《机器学习入门实战》：python2实现算法

## 参考书

+ 李航《统计学习》：比较系统，成体系，适合作为框架，遇到不会的去翻《西瓜书》，更为具体
+ 周志华《西瓜书》：经典之作，更为具体，可配套《南瓜书》一块看，内含具体公式推导
+ 博客 / github 上他人整理好的笔记

[机器学习学习路线参考](https://www.zhihu.com/question/20691338?sort=created)

## 代码框架

+ [scikit-learn](https://scikit-learn.org/stable/)：文档写得和教程一样，值得研究

# 基本概念

机器学习定义：常见两种定义

+ 不用显式编程就能教会机器做某件事
+ 经验E，任务T，性能P；根据经验E能改善性能P，更好的完成任务T

机器学习算法分类：

+ 监督学习（supervised learning）：带有标签数据，有反馈指导，正确答案的学习（有老师教）
  + 回归（regression）：预测连续值
  + 分类（classification）：预测离散值
+ 非监督学习（unsupervised learning）：无标签数据，自己学习，没有正确答案（自学成才）
  + 聚类（clustering）：音频噪声分离，多人说话声音分离，降噪，SVD一行代码octave
  + 非聚类
+ **如何处理无穷特征？**计算机空间有限，SVM 利用巧妙的数学方法处理

机器学习就是利用输入和输出的样例，构建输入与输出的关系，利用此关系来预测新输入对应的输出。

可以类比线性代数等式 `Y=X`，其中 `X` 为输入，`Y` 为输出。以此我们引出以下几个术语：

+ `Labels`(标签)：就是输出的 `Y`（**只能有一个吗？**）
+ `Features`(特征)：就是输入的`X`，大写`X`通常表示向量，展开为`{x1,x2,...,xn}`

+ `Model`(模型)：就是`Y=F(x)` 中的函数关系，模型就是指输入和输出的关系（规律）。
  + 模型存在两个状态
    + `Training`：训练模型，根据样例数据找到输入和输出的关系
    + `Inference`：模型推理，应用已经训练得到的关系对新输入进行预测，得到输出
  + 根据输出的值类型分类（连续/离散）：监督学习
    + `Regression`(回归)：预测输出的值是连续的（数不清楚）
      + 下个月百姓超市的苹果卖多少钱一斤？
      + 明天下雨概率是多少？
    + `Classification`(分类)：预测输出的值是离散的（数得清楚）
      + 图片里的是人，猫，狗？
      + 你是男人还是女人？

以上三个核心概念可看成抽象类，是一个壳/通式，需要实例化，于是出现了`Examples`

+ `Examples`(实例)：实例就是具体的数据，若类比数据库中表的概念，则`X和Y`是表头，那么`Examples`就是实际存在的数据。根据是否包含 `Labels` 分为两类：
  + `Labled example`：有标签数据，在**监督学习**中通常用于 `training` 阶段
  + `Unlabeld example`：无标签数据，非监督学习可用于 `training` ，其它均用于 `inference`

注意点

+ **特征选取**：特征要尽可能可量化，可观测。好的特征如长度，颜色，重量；不好特征的如美丽，漂亮这种主观性很强，不易量化和观测。

# 线性回归(Liner Regression)

## 基础概念模型

### 模型表达式

类比数学中的直线拟合问题，直观来说，就是给定若干个点，然你找出一条直线，使得每个点到达直线的距离之和最小。那么直线的表示就两个要素，斜率和截距，斜截式如下：
$$
y=kx+b
$$
而在机器学习中，通常习惯把**斜率**叫做**权重** `weight`，**截距**叫做**偏移量** `bias`，因此直线表达式可写成以下形式：
$$
y=b+w_1x_1
$$
这是最简单的情况，可将变量(feature)扩展到n个，所以线性回归**通用表达式**如下：
$$
y=b+w_1x_1+w_2x_2+...+w_nx_n=b+\mathop{\Sigma}_\limits{i=1}^nx_i
$$
因此给出数学中常用术语和机器学习术语对照表：

| 数学术语  | 机器学习术语 |
| --------- | ------------ |
| 变量X     | 特征Feature  |
| 因变量Y   | 标签Label    |
| 函数关系F | 模型Model    |
| 直线斜率K | 特征权重W    |
| 直线截距b | 偏移量bias   |

模型的训练过程就是根据已知数据(examples)计算出表达式中的最优权值和偏移量。

### 损失函数 lost function

那么如何计算最优呢？因此引入一个评价机制：损失函数 loss，来刻画估计值和真实值之间的偏差程度，该值越小说明估计值和真实值越接近，模型效果越好；反之说明效果差。

当模型训练到一定程度时即可将模型用于 Inference/Predicate 。loss过小会导致过拟合，即对新数据的预测能力很差；过大也是同样问题，因此选择合适的 loss 值，需要靠经验。

loss 函数有很多，最常见的是平方损失函数 Squared loss，即预测和实际值的差值的平方

```
Squared loss = (observation - predication(x))^2
```

对于所有样本则称作均方误差MSE(Mean Squared Error)，即每个样本的loss算术平均值
$$
MSE=\frac{1}{N}\sum\limits_{(x,y)\in D}(y-predication(x))^2
$$

+ N：是带标签的样本总个数
+ D：是数据集（样本集合）
+ (x,y)：是数据集的一个样本，x表示特征集合，y表示标签
+ predication表示模型（函数关系）

## 参数学习

我们的目标找到使得`MSE`最小时对应的参数，如何做到这点呢？常见算法如梯度下降

寻找最小值的过程好比从山顶走到一个山谷，在当前点，往四周查看那个方向下降最快，就往下降最快的方向走，以此迭代进行，直到抵达山谷。山谷可能不止一个，因此选择过程中任何一个方向的不同均可能导致到达不同的山谷（局部最优）。

### 梯度下降 Gradient Descent

步骤：

反复执行以下参数更新，直到收敛为止
$$
\theta_j=\theta_j- \alpha \frac{\partial }{\partial \theta_j }\!J(\theta_j)
$$

+ $\alpha$：学习率，规定恒为正数，决定了收敛的速度。取值过大过小都不好：
  + 学习率过小：收敛速度太慢，效率低
  + 学习率过大：收敛速度快，但可能永远无法收敛甚至发散
+ 学习率保持恒定，即不需要在迭代过程动态改变学习率大小，结构也会收敛。
  + 越接近最低点，说明斜率越接近0，因此每次下降斜率都在减小，因此步长在减小，会自然收敛
  + 到达**局部/全局最低点**后参数不会再改变，因为斜率为0

所有的参数必须同步更新，假如只有2个参数$\theta_0$和 $\theta_1$，见如下更新方式：
$$
\theta_0=\theta_0- \alpha \frac{\partial }{\partial \theta_0 }\!J(\theta_0)\\
\theta_1=\theta_1- \alpha \frac{\partial }{\partial \theta_1 }\!J(\theta_1)
$$
这种更新方式是错误的，因为所有参数不是同步更新，$\theta_0$更新后会直接影响$\theta_1$的更新。因此可增加中间变量先保存他们下一个的值，最后再一块赋值：
$$
tmp0=\theta_0- \alpha \frac{\partial }{\partial \theta_0 }\!J(\theta_0)\\
tmp1=\theta_1- \alpha \frac{\partial }{\partial \theta_1 }\!J(\theta_1)\\
\theta_0=tmp0\\
\theta_1=tmp1
$$
虽然两种方式都可能得到正确结果，但是第一种是不符合梯度下降逻辑的。

**batch** gradient descent：每次更新遍历**整个**数据集

### 加快梯度下降速度

**数据归一化**：特征值所取的范围相差太多将导致梯度下降速度太慢，因此可将其**均值正规化**，即所有的数据均值为0，其计算公式如下：
$$
x_i=\frac{x_i-u_i}{x_{max}-x_{min}}
$$
**学习率选择**：不同学习率对收敛速度有很大影响

+ **验证正确性**
  + **画图**：x轴为迭代次数，y轴为损失函数正确，画出收敛图像。若图像递减，说明正确迭代
  + **自动验证**：验证算法，设置一个阈值 $\epsilon$，当前后两个损失值相差小于 $\epsilon$ 表示收敛
+ **学习率大小**：以3倍的方式选择学习率，如`0.0001,0.0003,0.001,0.003,0.01,0.03,0.1`
  + **学习率过大**：图像难以收敛，甚至发散
  + **学习率过小**：收敛速度过慢

### 正规方程

直接解析解计算出参数结果，类似损失函数求导为0解出的参数值。
$$
\theta=(X^TX)^{-1}X^TY
$$
涉及矩阵求导，建议[深入推导](https://zhuanlan.zhihu.com/p/60719445)

## 矩阵计算

假设存在4组数据，X={1,2,3,4}，同时存在三组假设：
$$
h(x)=0.1+0.5x\\
h(x)=0.2+0.6x\\
h(x)=0.3+0.7x\\
$$


转为数组计算如下：同时计算三组假设，4组数据，效率极高
$$
\begin{bmatrix}
1&1 \\
1&2 \\
1&3 \\
1&4 \\
\end{bmatrix}
\begin{bmatrix}
0.1&0.2&0.3\\
0.5&0.6&0.7
\end{bmatrix}
=
\begin{bmatrix}
0.6&0.8&1.0 \\
1.1&1.4&1.7 \\
1.6&2.0&2.4 \\
2.1&2.6&3.1 \\
\end{bmatrix}
$$
特性：

+ 满足结合性，但不满足交换性
+ 单位矩阵I：`AI=IA`
+ 逆矩阵：当一个矩阵是方阵且不是全零矩阵时，它存在逆矩阵。$AA^{-1}=A^{-1}A=I$
+ 奇异矩阵：不存在逆矩阵的矩阵称作奇异矩阵
+ 矩阵装置：矩阵第i行转换为第i列（第一个元素和最后一个元素连线作为旋转轴，旋转180度）

## 多元线性回归

变量从原来的1个变为 n 个，仅此而已，可借用矩阵简化表达式
$$
h_\theta(x)=\theta_0+\theta_1x_1+\theta_2x_2+...+\theta_nx_n
=\begin{bmatrix}
\theta_0 &\theta_1 &\cdots &\theta_n 
\end{bmatrix}\begin{bmatrix}
x_0 \\
x_1 \\
\vdots  \\
x_n \\
\end{bmatrix}
=
\theta^TX
$$
其中 $x_0=1$，便于计算 $\theta_0$

以下是一些符号约定：

+ $m$：数据样本个数（行数）
+ $n$：数据样本的特征个数（列数）
+ $x^{(i)}$：第 i 个数据样本（第 i 行）
+ $x^{(i)}_j$：第 i 个数据样本的第 j 个特征（第 i 行第 j 列）

## 多项式回归

变量的幂次可为1，2， 3，...，n次方，因此不局限于线性回归只能拟合直线模型，还可以拟合任意曲线。

常见拟合模型如下

+ 增加高次幂：$h_\theta(x) =\theta_0+\theta_1x_1+\theta_2x_2^2+...+\theta_nx_n^n$
+ 增加平方根：使得曲线增长不会过快

注意特征的范围，需要先进行归一化